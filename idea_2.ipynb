{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296e16ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akaze\\Documents\\VS Code\\NTO\\nto_neri\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebca156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, numeric_estimator=None):\n",
    "        self.num_cols = []\n",
    "        self.cat_cols = []\n",
    "        self.cat_categories = {}\n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.numeric_estimator = numeric_estimator\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        # store categories per categorical column (train categories only)\n",
    "        for c in self.cat_cols:\n",
    "            vals = X[c].dropna().astype(str).unique().tolist()\n",
    "            self.cat_categories[c] = vals\n",
    "\n",
    "        # Iterative imputer with tree estimator (ExtraTreesRegressor) for numeric features\n",
    "        estimator = self.numeric_estimator or ExtraTreesRegressor(n_estimators=30, n_jobs=-1, random_state=42)\n",
    "        self.imputer = IterativeImputer(estimator=estimator, max_iter=10, random_state=42)\n",
    "        if self.num_cols:\n",
    "            self.imputer.fit(X[self.num_cols])\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.imputer.transform(X[self.num_cols]))\n",
    "        else:\n",
    "            self.imputer = None\n",
    "            self.scaler = None\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        Xc = X.copy()\n",
    "        # categorical -> codes using training categories, unseen -> -1\n",
    "        for c in self.cat_cols:\n",
    "            Xc[c] = Xc[c].astype(str).where(Xc[c].notnull(), np.nan)\n",
    "            Xc[c] = pd.Categorical(Xc[c], categories=self.cat_categories[c]).codes\n",
    "            # pandas codes: -1 for missing/unseen\n",
    "            Xc[c] = Xc[c].astype(int)\n",
    "\n",
    "        # numeric imputation + scaling\n",
    "        if self.num_cols:\n",
    "            X_num = self.imputer.transform(Xc[self.num_cols])\n",
    "            X_num = self.scaler.transform(X_num)\n",
    "            Xc[self.num_cols] = X_num\n",
    "\n",
    "        # return with stable column order\n",
    "        return Xc[self.num_cols + self.cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70b197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(obj, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Training + Optuna\n",
    "# -------------------------\n",
    "def main(args):\n",
    "    out_dir = Path(args.output_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # load\n",
    "    train = pd.read_csv(args.train)\n",
    "    test = pd.read_csv(args.test)\n",
    "    sample_sub = pd.read_csv(args.sample_submission)\n",
    "\n",
    "    id_col = args.id_col\n",
    "    target_col = args.target_col\n",
    "    features = [c for c in train.columns if c not in [id_col, target_col]]\n",
    "\n",
    "    X_df = train[features].copy()\n",
    "    y = train[target_col].copy().astype(str)\n",
    "    X_test_df = test[features].copy()\n",
    "\n",
    "    # encode class names (save mapping)\n",
    "    classes = sorted(y.unique().tolist())\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    y_idx = y.map(class_to_idx).values\n",
    "    joblib.dump(classes, out_dir / \"class_names.joblib\")\n",
    "\n",
    "    # Optuna objective\n",
    "    def objective(trial):\n",
    "        # hyperparams\n",
    "        params = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"num_class\": len(classes),\n",
    "            \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"goss\"]),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 256),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 200),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "            \"random_state\": 42,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": -1\n",
    "        }\n",
    "        if params[\"boosting_type\"] == \"gbdt\":\n",
    "            params[\"bagging_fraction\"] = trial.suggest_float(\"bagging_fraction\", 0.5, 1.0)\n",
    "            params[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 1, 7)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=args.n_splits, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        for fold, (tr_idx, val_idx) in enumerate(skf.split(X_df, y_idx)):\n",
    "            X_tr_df = X_df.iloc[tr_idx].reset_index(drop=True)\n",
    "            X_val_df = X_df.iloc[val_idx].reset_index(drop=True)\n",
    "            y_tr = y_idx[tr_idx]\n",
    "            y_val = y_idx[val_idx]\n",
    "\n",
    "            # fit preprocessor on train fold only (no leakage)\n",
    "            pre = Preprocessor()\n",
    "            pre.fit(X_tr_df)\n",
    "            X_tr = pre.transform(X_tr_df)\n",
    "            X_val = pre.transform(X_val_df)\n",
    "\n",
    "            # oversample train fold (SMOTE) to help rare classes\n",
    "            try:\n",
    "                sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "                X_tr_res, y_tr_res = sm.fit_resample(X_tr, y_tr)\n",
    "            except Exception:\n",
    "                # fallback if SMOTE fails\n",
    "                X_tr_res, y_tr_res = X_tr, y_tr\n",
    "\n",
    "            # model\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=args.n_estimators)\n",
    "            callbacks = []\n",
    "            if fold == 0:\n",
    "                callbacks.append(LightGBMPruningCallback(trial, \"multi_logloss\"))\n",
    "            model.fit(\n",
    "                X_tr_res, y_tr_res,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric=\"multi_logloss\",\n",
    "                callbacks=callbacks,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            y_val_pred = model.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_val_pred, average=\"macro\")\n",
    "            fold_scores.append(f1)\n",
    "\n",
    "        mean_f1 = float(np.mean(fold_scores))\n",
    "        # we minimize (1 - F1)\n",
    "        return 1.0 - mean_f1\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), study_name=args.study_name, storage=None)\n",
    "    study.optimize(objective, n_trials=args.n_trials, show_progress_bar=True)\n",
    "\n",
    "    print(\"Best value (1 - F1):\", study.best_value)\n",
    "    print(\"Best params:\", study.best_params)\n",
    "    joblib.dump(study, out_dir / \"optuna_study.joblib\")\n",
    "\n",
    "    # Fit final preprocessor on full train and save\n",
    "    pre_full = Preprocessor()\n",
    "    pre_full.fit(X_df)\n",
    "    X_full = pre_full.transform(X_df)\n",
    "    X_test = pre_full.transform(X_test_df)\n",
    "    joblib.dump(pre_full, out_dir / \"preprocessor.joblib\")\n",
    "\n",
    "    # Retrain top-K trials on full data and save models\n",
    "    top_k = args.top_k\n",
    "    trials_sorted = sorted([t for t in study.trials], key=lambda t: t.value)[:top_k]\n",
    "    model_paths = []\n",
    "    for i, t in enumerate(trials_sorted):\n",
    "        params = t.params.copy()\n",
    "        params.update({\"objective\": \"multiclass\", \"num_class\": len(classes), \"random_state\": 42, \"n_jobs\": -1, \"verbose\": -1})\n",
    "        # ensure bagging params exist for gbdt\n",
    "        model = lgb.LGBMClassifier(**params, n_estimators=args.n_estimators)\n",
    "        # try SMOTE on full (may be beneficial)\n",
    "        try:\n",
    "            sm = SMOTE(random_state=42, n_jobs=-1)\n",
    "            X_res, y_res = sm.fit_resample(X_full, y_idx)\n",
    "        except Exception:\n",
    "            X_res, y_res = X_full, y_idx\n",
    "\n",
    "        model.fit(X_res, y_res)\n",
    "        path = out_dir / f\"model_top_{i}.joblib\"\n",
    "        joblib.dump(model, path)\n",
    "        model_paths.append(str(path))\n",
    "        print(f\"Saved model {path}\")\n",
    "\n",
    "    joblib.dump(model_paths, out_dir / \"model_paths.joblib\")\n",
    "    print(\"All done. Artifacts in:\", out_dir)\n",
    "\n",
    "    # optional: build averaged predictions and save submission\n",
    "    probs = None\n",
    "    for p in model_paths:\n",
    "        m = joblib.load(p)\n",
    "        pr = m.predict_proba(X_test)\n",
    "        probs = pr if probs is None else probs + pr\n",
    "    probs = probs / len(model_paths)\n",
    "    pred_idx = np.argmax(probs, axis=1)\n",
    "    preds = [classes[i] for i in pred_idx]\n",
    "\n",
    "    submission = pd.DataFrame({id_col: test[id_col]})\n",
    "    for cls in classes:\n",
    "        submission[cls] = 0\n",
    "    for i, cls in enumerate(preds):\n",
    "        submission.loc[i, cls] = 1\n",
    "    out_sub = out_dir / \"submission_ensemble.csv\"\n",
    "    submission.to_csv(out_sub, index=False)\n",
    "    print(\"Saved ensemble submission:\", out_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a869236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train TRAIN] [--test TEST]\n",
      "                             [--sample_submission SAMPLE_SUBMISSION]\n",
      "                             [--output_dir OUTPUT_DIR] [--id_col ID_COL]\n",
      "                             [--target_col TARGET_COL] [--n_trials N_TRIALS]\n",
      "                             [--n_splits N_SPLITS]\n",
      "                             [--n_estimators N_ESTIMATORS] [--top_k TOP_K]\n",
      "                             [--study_name STUDY_NAME]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\akaze\\AppData\\Roaming\\jupyter\\runtime\\kernel-v388c161f59f508fa1ff27a0ce1de1c2985dde2a14.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akaze\\Documents\\VS Code\\NTO\\nto_neri\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train\", default=\"train.csv\")\n",
    "    parser.add_argument(\"--test\", default=\"test.csv\")\n",
    "    parser.add_argument(\"--sample_submission\", default=\"sample_submission.csv\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"artifacts\")\n",
    "    parser.add_argument(\"--id_col\", default=\"idx\")\n",
    "    parser.add_argument(\"--target_col\", default=\"type\")\n",
    "    parser.add_argument(\"--n_trials\", type=int, default=60)\n",
    "    parser.add_argument(\"--n_splits\", type=int, default=5)\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=1500)\n",
    "    parser.add_argument(\"--top_k\", type=int, default=3)\n",
    "    parser.add_argument(\"--study_name\", type=str, default=\"astro_study\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
